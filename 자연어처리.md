# 임베딩

* 단어나 문장을 벡터로 바꾸는 과정
* 인간의 언어 > 컴퓨터 언어
* Word2Vec 임베딩

* 벡터 연산(유추 평가) 아들 - 딸 = 소년 - 소녀
* 문서에 속한 단어가 유사하면 문서의 의미도 비슷하다 (문서에 단어를 벡터를 바꿀 경우)
* 단어가 어떤 순서로 나타나는지 살핀다 > 시퀸스 정보에 의미가 녹아 있다
* Bag of Words 가정
* Language Model: 확률모델
* 전이학습: 다른 딥러닝 모델의 입력값
* f(w1, w2 ... wn) = prob
* P(A|B) = P(A, B) / P(B) -> B 다음에 A가 나올 확률


## 단어가 어떤 단어와 주로 같이 나타나는지 살핀다 > 문맥에 의미가 녹아 있다
* ELMo, BERT
* https://github.com/ratsgo/embedding

# 단어 임베딩 구축 (워드 투데이)
* 어떤 패키지의 도움 받지 않고 행렬로만 한다 - 단어수준의 임베딩 만들기

## 뉴럴넷 기초
뉴럴렛: (Directed Acyclic Graph - 시작과 끝 노드가 있다)
* Deep Neural Networks/뉴럴렛 = Directed Acyclic Graph + Probability Model
* 판단기준은 학습 손실
* Backpropagation: 체인룰을 이용해서 학습 손실 최소화  
    * softmax + Cross entropy Mode (미분해서 최대최소)

# Word2Vc
* Skip-Gram Model
    * 타겟 단어로 문맥 단어를 맞추는 과정에서 학습
    
